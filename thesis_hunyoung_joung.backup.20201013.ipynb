{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of South Korean Journalism Framing on COVID-19 and 21st South Korea General Election\n",
    "## :Big Data, Word Embeddings, and Sentiment Analysis Leveraging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 목적\n",
    "    범주화, 프레임은 범주화된 단편적인 ‘진실’이다. 사회에는 다양한 ‘진실’이 공존하여야 한다. 특정 집단이 추구하는 진실만이 우위를 구가(謳歌)하는 사회는 더이상 평등하고, 공정한 사회라고 부를수 없다. 뉴스콘텐츠또한 다양한 프레임으로 구성되어야 한다. 극단적인 소수의 프레임이 다수의 뉴스콘텐츠를 지배한다면, 언론은 더이상 진실을 보도하지 않는다고 할 수 있고, 대중의 신뢰는 멀어질 것이다. 하지만, 제기된 문제점들을 살펴 보았을 때, 한국 언론의 정지척 편향성에 대한 거대담론은 보수와 진보집단간의 ‘공격 저널리즘’의 악순환을 되풀이 하는 것으로 보여지고 있으며, 정치 이념적 프레임이 정치적 사안을 벗어나, 사회, 문화, 경제전반에 걸쳐 지배적인 프레임으로 비춰진다(최영재,2011;현기득·서미혜,2019). 언론의 정치적 특질을 부정하지는 않는다. 다만, 과잉된 정치 프레임전쟁의 양상이 정치적 사안을 넘어, 사회전반의 뉴스콘텐츠에 영향력을 행사할 때, 진영논리로 비화하는 것은 경계해야 한다. 따라서, 본 연구에서는 이러한 한국 언론의 실태를 보다 기계적으로 검증할 수 있다는것에서 그 필요성이 있다고 할 수 있다. 또한, 본 연구의 기본적인 목적은 기계적인 프레임분석방법을 통하여, 보다 면밀하게 프레임을 분석하는 것이며, 부차적으로는 추출된 각 프레임의 기계적인 해석을 통하여, 상호 비교하는 것이다.\n",
    "\n",
    "### 2. 연구문제\n",
    "    1. 각 언론사별 생산하는 ‘제21대 대한민국 총선’뉴스와 ‘코로나19사태’뉴스에서 추출한 프레임의 주제유목(categorize)은 ‘원인’, ‘결과’, ’대책’으로 구분할 수 있는가?\n",
    "    2. 각 언론사별 생산하는 ‘제21대 대한민국 총선’뉴스와 ‘코로나19사태’뉴스에서 추출한 프레임의 주제유목은 서로 유사한가?\n",
    "    3. 각 언론사별 생산하는 ‘제21대 대한민국 총선’뉴스와 ‘코로나19사태’뉴스에서 추출한 프레임의 주제유목은 아래의 이야기(narrative) 구조 주제유목과 유사한가?\n",
    "        a. 내용: 코로나및 경제위기등에 대한 위험성을 강조\n",
    "        b. 내용: ‘원인’으로 대통령 및 정권여당의 실정(失政)을 강조, 또는 야당의 국정발목잡기를 강조\n",
    "        c. 내용: ‘대책’ 및 ‘결과’로써, 국정안정을 위해 여당지지 또는 야당지지\n",
    "\n",
    "### 3. 연구방법\n",
    "    1. 연구대상\n",
    "        빅카인즈로(https://www.bigkinds.or.kr/)로 부터 추출한 뉴스콘텐츠를 연구의 대상으로 한다.\n",
    "        구체적인 대상의 조건은 아래와 같다.\n",
    "        (1)\t연구대상 \n",
    "            1) 공통조건\n",
    "                ① 기간: 2019년12월1일∼2020년6월30일\n",
    "                ② 형식: 모든 기사형식\n",
    "                ③ 대상언론: 전국단위 일간지(경제지 및 전문지포함), 지상파방송사\n",
    "                (조선일보, 경향신문, 국민일보, 내일신문, 동아일보, 문화일보, 한국일보, 중앙일보, 한겨례신문, 서울일보, 매일경제, 서울경제, 머니투데이, 아시아경제, 아주경제, 한국경제, 헤럴드경제, 파이낸셜뉴스, 디지털타임스, 전자신문, KBS, SBS, MBC)\n",
    "            2)‘제21대 대한민국 총선’ 뉴스콘텐츠 조건: Query = (총선 OR 국회의원선거 OR 21대 선거 OR 국회의원 선거)\n",
    "            3)‘코로나19사태’ 뉴스콘텐츠 조건: Query = (코로나 OR 우한 OR COVID OR COVID-19)\n",
    "        (2)\t취득 데이터 구조 (Low Data Structure)\n",
    "            뉴스 식별자,일자,언론사,기고자,제목,통합 분류1,통합 분류2,통합 분류3,사건/사고 분류1,사건/사고 분류2,사건/사고 분류3,인물,위치,기관,키워드,특성추출,본문,URL,분석제외 여부\n",
    "    2. 연구도구\n",
    "        뉴스콘텐츠의 취득 및 처리, 결과도출은 모두 컴퓨터 프로그램의 알고리즘으로 진행된다.\n",
    "        Jupyter Notebook 기반으로 Program언어로는 Python을 이용하며, 언어모델 (Language Model)을 구축하고, 연구대상을 언어모델에 대입하여, 토픽 모델링 (Topic Modeling)과 워드임베딩 (Word Embedding)의 두 가지 방법으로 결과를 도출하고, 각각의 결과를 SVM (support vector machine)을 이용하여 비교분석한다.\n",
    "        (1) 언어모델 (Language Model)\n",
    "            Word2vec\n",
    "        (2) 토픽 모델링 (Topic Modeling)\n",
    "            LDA (Latent Dirichlet Allocation)\n",
    "        (3) 워드임베딩 (Word Embedding)\n",
    "            Word2vec의 예측기반 (Prediction Based Embedding)알고리즘을 사용.\n",
    "            단어사용의 빈도수를 사용하지 않으며, 특정한 단어의 좌우의 (n)단어까지 유의단어로 차원을 할당하여, 다른 단어들과 차원을 중복시켜, 유의성에 대한 선형성을 도출한다.\n",
    "                ① CBOW (Continuous Bag of Words)\n",
    "                ② Vector size=(n) dimension\n",
    "                ③ Window=R(n), L(n), total 2(n) token\n",
    "                ④ frequency=10 times loop\n",
    "                ⑤ CPU=4Q / GPU=CUDA core 1,920\n",
    "                ⑥ Training count=(n) times loop\n",
    "        (4) SVM (support vector machine)\n",
    "            주어진 변수의 차원을 2차원평면으로 치환하여 최적화된 초평면으로 귀속시켜, 연구문제의 결과를 도출한다.\n",
    "                ① 초평면 2차원분석\n",
    "                ② 로지스틱 회귀분석\n",
    "    3. 연구절차 및 자료수집\n",
    "        (1) 제약조건 (constraints)\n",
    "            사전탐색을 통하여 연구대상의 훈련데이터 (Training data)가 코로나19사태 (300MB), 제21대 대한민국 총선 (200MB)정도로, 각각의 데이터의 양이 작아, 기계학습후에 정확성을 확보하기 어렵다. 따라서, 언어모델을 이용한 기존 단어의 선형성을 재사용하여 정확성을 담보하고자 한다.\n",
    "        (2) 언어모델(Language Model)\n",
    "            언어모델의 구축은 비교적 편향성이 적은 한글위키를 대상으로 구축한다.\n",
    "            1) https://dumps.wikimedia.org/kowiki/latest/ 로 부터 한글위키데이터를 수집 (Acquisition)\n",
    "            2) 점검 및 탐색 (Inspection and exploration)\n",
    "            3) 전처리 및 정제 (Preprocessing and Cleaning)\n",
    "            4) 모델링 및 훈련 (Modeling and Training) \n",
    "                Word2vec을 통하여 학습되며, 밀집표현 (distributed/Dense representations)으로 생성\n",
    "                ① Training data set (60%)\n",
    "                ② Testing data set (25%)\n",
    "                ③ Validation data set (15%)\n",
    "            5) 평가 (Evaluation)\n",
    "            6) 배포 (Deployment)\n",
    "        (3) 기계학습을 이용한 프레임분석 \n",
    "            1) 분석대상 뉴스콘텐츠는 빅카인즈로(https://www.bigkinds.or.kr/)부터 추출 (Acquisition)\n",
    "            2) 추출 대상뉴스콘텐츠는 ‘코로나19사태’에 관련된 키워드 조합이다.\n",
    "            3) 추출 대상뉴스콘텐츠는 ‘제21대 대한민국 총선’에 관련된 키워드 조합이다.\n",
    "            4) 2)의 과정에서 추출한 데이터의 점검 및 탐색(Inspection and exploration)\n",
    "            5) 3)의 과정에서 추출한 데이터의 점검 및 탐색(Inspection and exploration)\n",
    "            6) 2)의 과정에서 추출한 데이터의 전처리 및 정제 (Preprocessing and Cleaning)\n",
    "            7) 3)의 과정에서 추출한 데이터의 전처리 및 정제 (Preprocessing and Cleaning)\n",
    "            8) 코로나19사태, 6)의 데이터의 모델링 및 훈련 (Modeling and Training) \n",
    "                Word2vec을 통하여 학습되며, 밀집표현(distributed/Dense representations)으로 생성된다.\n",
    "                ① Training data set (80%)\n",
    "                ② Testing data set (20%)\n",
    "            9)제21대 대한민국 총선, 7)의 데이터의 모델링 및 훈련 (Modeling and Training) \n",
    "                Word2vec을 통하여 학습되며, 밀집표현(distributed/Dense representations)으로 생성된다.\n",
    "                ① Training data set (80%)\n",
    "                ② Testing data set (20%)\n",
    "            10)\t8)과 9)의 데이터에서 상위어휘들을 추출한다.\n",
    "            11)\t10)의 데이터는 각 사안의 프레임이다.\n",
    "            12)\t8)과 9)의 데이터에서 SVM을 통한 유사도를 측정한다.\n",
    "            13)\t8)의 데이터에서 이야기구조모델 통한 유사도를 측정한다.\n",
    "            14)\t9)의 데이터에서 이야기구조모델 통한 유사도를 측정한다.\n",
    "            15)\t12), 13), 14)의 유사도가 비슷하다면, 프레임은 정치이념적 이라고 할 수 있다.\n",
    "            16)\t13)의 유사도가 상위값을 가질 경우 ‘제21대 대한민국 총선’뉴스는 진보성향이다.\n",
    "            17)\t14)의 유사도가 상위값을 가질 경우 ‘코로나19사태’뉴스는 진보성향이다.\n",
    "            \n",
    "        (4) 주제모델 이용한 프레임분석 \n",
    "            1) 분석대상 뉴스콘텐츠는 빅카인즈로(https://www.bigkinds.or.kr/)부터 추출 (Acquisition)\n",
    "            2) 추출 대상뉴스콘텐츠는 ‘코로나19사태’에 관련된 키워드 조합이다.\n",
    "            3) 추출 대상뉴스콘텐츠는 ‘제21대 대한민국 총선’에 관련된 키워드 조합이다.\n",
    "            4) 2)의 과정에서 추출한 데이터의 점검 및 탐색(Inspection and exploration)\n",
    "            5) 3)의 과정에서 추출한 데이터의 점검 및 탐색(Inspection and exploration)\n",
    "            6) 2)의 과정에서 추출한 데이터의 전처리 및 정제 (Preprocessing and Cleaning)\n",
    "            7) 3)의 과정에서 추출한 데이터의 전처리 및 정제 (Preprocessing and Cleaning)\n",
    "            8) 이하 TODO\n",
    "    4. 자료분석방법\n",
    "        (1)\t각 언론이 생산하는 ‘제21대 대한민국 총선’뉴스콘텐츠의 프레임 추출\n",
    "        ‘위험,‘원인’,‘대책’에 관련된 유의어를 추출후, 밀집도를 표현하였을 때의 밀도함수 표현을 수치 및 그래프로 표현. 상위100위의 밀도를 가진 ‘단어’의 조합을 평가하여, 프레임을 추출.\n",
    "        (2)\t각 언론이 생산하는 ‘코로나19사태’뉴스콘텐츠의 프레임 추출\n",
    "        ‘위험’,‘원인’,‘대책’에 관련된 유의어를 추출후, 밀집도를 표현하였을 때의 밀도함수 표현을 수치 및 그래프로 표현. 상위100위의 밀도를 가진 ‘단어’의 조합을 평가하여, 프레임을 추출.\n",
    "        (3)\t각 언론이 생산하는 뉴스콘텐츠에서 추출한 프레임의 유사도 분석\n",
    "        ‘제21대 대한민국 총선’ 와 ‘코로나19사태’에서 추출된 상위100의 유사도를 SVM 으로 도출하며, 보조적으로 로지스틱 회귀분석을 이용하여 검증한다. \n",
    "        (4)\t각 언론이 생산하는 뉴스콘텐츠에서 추출한 프레임의 ‘국정안정’을 지지도 분석\n",
    "        ‘제21대 대한민국 총선’ 와 ‘코로나19사태’에서 추출된 상위100의 프레임이 ‘국정지지’에대한 긍정적단어로 구성되는 지를 분석한다.\n",
    "        \n",
    "        \n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "   \n",
    "2. Data model구축\n",
    " Jupyter에서 Python을 이용하여 초기 Model을 구축하였고, 최소 1번 Keyword를 훈련 하였다. \n",
    " 또한, log는 최소한으로 표시한다.\n",
    " \n",
    " 2.1. Library 취득\n",
    "  System library를 통하여, jpype를 비롯한 각각의 필요한 Library들을 취득하였다.    \n",
    "  취득한 Library를 import하고, 필요한 함수들을 정의하였다.\n",
    "  \n",
    "  \n",
    "  \n",
    " 2.2. Low data 취득\n",
    "  Pandas를 이용하여, \"./crawl_result.tsv\"를 취득하여, Low dataset을 작성하였고, 각각의 데이터에 Head명을 부여하였다.\n",
    "  \n",
    " 2.3. Tokenizing & Training dataset & Test dataset 작성\n",
    "  konlpy를 이용하여, 각각의 Data에서 Token을 추출하고,\n",
    "  Low dataset의 \"contents\"를 Training dataset으로 작성하였고,\n",
    "  Low dataset의 \"title\"+\"contents\"를 Test dataset으로 작성하였다.\n",
    "  한편, Low data취득시에 Separator로 사용한 \"||\"를 제거하였다.\n",
    " \n",
    " 2.4. Model 작성\n",
    "  Tranning dataset을 Word2Vector를 사용하여 아래와 같이 작성하였다.\n",
    "  Vector크기=100차원\n",
    "  Window=좌우 5 Token까지\n",
    "  frequency=5회반복\n",
    "  CPU=4개 사용  \n",
    "  Training count=80회반복\n",
    "  초기 Model이 Test dataset보다 커서, Common Bag Of Words CBOW Model대신, Skip Gram을 사용하였다.\n",
    " \n",
    " 2.5. 훈련전 검증 및 시각화\n",
    "  Model dataset을 작성하였고, \n",
    "  초기Model과 훈련후의 Test dataset의 Accuracy 를 비교하기 위해, \n",
    "  이에 대한 Sample similarity을 출력해보았으며, 시각화하였다.\n",
    " \n",
    " 2.7. Training\n",
    "  Model dataset에 Vector화된 Test dataset를 추가하였다.\n",
    "  \n",
    " 2.8. 훈련후 검증 및 시각화\n",
    "   - SVM\n",
    "   \n",
    "3. Conclusion    \n",
    " Model 학습 후 유효성 검사 세트의 정확성\n",
    "   - SVM\n",
    "   \n",
    "첫번째 고민, 기계가 인간의 의도를 이해하여야 프레임을 추출 할 수 있는 것인가?\n",
    "그렇다면, 이해해야할 기사의 쟁점을 이해해야 하는 가, 아니면 \n",
    "\n",
    "인간: 내용의 이해 -> 중심주장을 추출 -> 의도의 파악\n",
    "기계: 문장의 중심을 추출 -> 구조의 파악\n",
    "\n",
    "두번째 고민, 각 프레임 분석과 기계분석을 대입가능한지 여부 판독.\n",
    "bert에 대한 고민.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps in 1: Initiative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anaconda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1.1)\n",
      "Requirement already satisfied: gensim in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.19.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.9.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: boto3 in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.10.19)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (1.13.19)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.14.0,>=1.13.19->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.14.0,>=1.13.19->boto3->smart-open>=1.8.1->gensim) (0.14)\n",
      "Requirement already satisfied: anaconda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1.1)\n",
      "Requirement already satisfied: BeautifulSoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from BeautifulSoup4) (1.8)\n",
      "Requirement already satisfied: anaconda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1.1)\n",
      "Requirement already satisfied: lxml in c:\\programdata\\anaconda3\\lib\\site-packages (4.3.4)\n",
      "Requirement already satisfied: anaconda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1.1)\n",
      "Requirement already satisfied: konlpy in c:\\programdata\\anaconda3\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: JPype1>=0.5.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in c:\\programdata\\anaconda3\\lib\\site-packages (from JPype1>=0.5.7->konlpy) (3.7.4.2)\n",
      "Requirement already satisfied: anaconda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.19.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
      "Requirement already satisfied: anaconda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.19.1)\n",
      "Requirement already satisfied: anaconda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.19.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
      "Requirement already satisfied: anaconda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (0.21.2)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.19.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (0.13.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.1)\n",
      "Requirement already satisfied: anaconda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1.1)\n",
      "Requirement already satisfied: wordcloud in c:\\programdata\\anaconda3\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (3.1.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (1.19.1)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (6.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (41.0.1)\n",
      "Requirement already satisfied: anaconda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1.1)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.19.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-12 10:14:36 - thesis_hunyoung_joung - INFO - BEGIN \n",
      "2020-10-12 10:14:36 - thesis_hunyoung_joung - INFO - Default path? C:\\Users\\root\\Documents\\0. The Cyber University of Korea\\Graduated School\\Semester 3\\Thesis\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Get libraries and install\n",
    "\n",
    "Define libraries and make alias\n",
    "Define global variable as like final or static\n",
    "'''\n",
    "import sys\n",
    "!{sys.executable} -m pip install anaconda gensim\n",
    "!{sys.executable} -m pip install anaconda BeautifulSoup4\n",
    "!{sys.executable} -m pip install anaconda lxml\n",
    "!{sys.executable} -m pip install anaconda konlpy\n",
    "!{sys.executable} -m pip install anaconda pandas\n",
    "!{sys.executable} -m pip install anaconda numpy\n",
    "!{sys.executable} -m pip install anaconda pandas\n",
    "!{sys.executable} -m pip install anaconda scikit-learn\n",
    "!{sys.executable} -m pip install anaconda wordcloud\n",
    "!{sys.executable} -m pip install anaconda matplotlib\n",
    "\n",
    "import os \n",
    "import gc\n",
    "import jpype\n",
    "import konlpy\n",
    "import gensim\n",
    "import logging\n",
    "import warnings\n",
    "import codecs\n",
    "import re\n",
    "import glob\n",
    "import requests\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from glob import glob\n",
    "from pandas import Series, DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "global DEFAULT_FONT                  # Default font\n",
    "global DEFAULT_PATH                  # Default file path\n",
    "global DEFAULT_OUTPUT_PATH          # Default output file path\n",
    "global LOW_MODELDATA_PATH            # Low level model data path\n",
    "global LOW_TRAININGDATA_PATH         # Low level training data path\n",
    "global LOW_TRAININGDATA\n",
    "global LOW_TRAININGDATA4_C19\n",
    "global LOW_TRAININGDATA4_ELECTION\n",
    "global LOW_MODELDATA                 # Low level training data file\n",
    "global DEFAULT_ENCODE                # For byte\n",
    "global hangul_encoding               # For file name and contents\n",
    "global jap_encoding                  # For OS path\n",
    "global covid_df                      # COVID19 dataframe\n",
    "global election_df                   # Election dataframe\n",
    "global LINE_FEED                     # Line feed\n",
    "\n",
    "\n",
    "# Default paths for Japanese Windows version\n",
    "DEFAULT_PATH = r'C:\\Users\\root\\Documents\\0. The Cyber University of Korea\\Graduated School\\Semester 3\\Thesis'\n",
    "DEFAULT_FONT = r'C:/Windows/Fonts/gulim.ttc'\n",
    "DEFAULT_OUTPUT_PATH = 'output'\n",
    "LOW_TRAININGDATA_PATH = 'data'\n",
    "LOW_MODELDATA_PATH = 'low_model_data'\n",
    "LOW_MODELDATA = 'model_data.txt'\n",
    "LOW_TRAININGDATA = 'training_data.csv'\n",
    "LOW_TRAININGDATA4_C19 = 'training_data_c19.csv'\n",
    "LOW_TRAININGDATA4_ELECTION = 'training_data_election.csv'\n",
    "LINE_FEED = '\\n'\n",
    "\n",
    "# Default encoding\n",
    "DEFAULT_ENCODE = 'UTF-8' \n",
    "# Encode for Hangul 'cp949'\n",
    "hangul_encoding = 'cp949'\n",
    "# Encode for Japnese 'cp932'\n",
    "jap_encoding = 'cp932'\n",
    "\n",
    "# Set log\n",
    "log = logging.getLogger(\"thesis_hunyoung_joung\")\n",
    "log.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s',\"%Y-%m-%d %H:%M:%S\")\n",
    "log.propagate = True\n",
    "# Output to console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(formatter)\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "# Output to file\n",
    "file_handler = logging.FileHandler(filename = os.path.join(DEFAULT_PATH, LOW_TRAININGDATA_PATH, DEFAULT_OUTPUT_PATH, \"thesis_hunyoung_joung_run.log\"))\n",
    "file_handler.setFormatter(formatter)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "# Record logs to file and show to console\n",
    "log.addHandler(console_handler)\n",
    "log.addHandler(file_handler)\n",
    "# 80 length\n",
    "log.info(\"BEGIN \")\n",
    "log.info(\"Default path? {}\".format(DEFAULT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-13 03:43:02 - thesis_hunyoung_joung - INFO - training_data_c19.csv: Data shape? Column count=19items, Row count=173,567lines\n",
      "2020-10-13 03:43:02 - thesis_hunyoung_joung - INFO - training_data_c19.csv: Data size? Total=440.4476MB=\n",
      "2020-10-13 03:43:02 - thesis_hunyoung_joung - INFO - training_data_c19.csv: Each data size? Total=45.2162MB, Target(id=4.2616MB, date=1.3131MB, media=0.6942MB, author=1.2723MB, title=5.2483MB, contents=32.4267MB)\n",
      "2020-10-13 03:43:02 - thesis_hunyoung_joung - INFO - 31.706781819124604\n",
      "2020-10-13 03:43:02 - thesis_hunyoung_joung - INFO - 161\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Commons\n",
    "'''\n",
    "def file_read_support(file):\n",
    "    with open(file, 'r', encoding=DEFAULT_ENCODE) as f: \n",
    "        #soup = BeautifulSoup(f, \"lxml\")\n",
    "        #words = soup.find_all(re.compile(\"[doc]\"))\n",
    "        words = f.read()\n",
    "        spaces = words.split()\n",
    "        words_len = 0\n",
    "        for i in spaces:\n",
    "            words_len += len(i)\n",
    "    return words, len(spaces), words_len\n",
    "'''\n",
    "Training data structure\n",
    "ID,일자,언론사,기고자,제목,통합 분류1,통합 분류2,통합 분류3,사건/사고 분류1,사건/사고 분류2,사건/사고 분류3,\n",
    "인물,위치,기관,키워드,특성추출,본문,URL,분석제외여부\n",
    "\n",
    "'id','date','media','auth','title','category1','category2','category3','incident1','incident2','incident3',\n",
    "'rel_person','rel_location','rel_org','key_words','feature','contents','url','flag'\n",
    "'''\n",
    "\n",
    "'''\n",
    "Input: O/S path + file name \n",
    "Output: shape(column count,row count),data size,target data(length(avg,max,min),word frequency,) 4 and 16 column\n",
    "'''\n",
    "def csv_file_analysis_support(file):\n",
    "    # Low data size\n",
    "    data_size = os.path.getsize(file)/(1024.0000 * 1024.0000)\n",
    "    # Low data shape\n",
    "    col_cnt = 0\n",
    "    row_cnt = 0\n",
    "    # Acquired target data size list\n",
    "    id_size_list = []\n",
    "    date_size_list = []\n",
    "    media_size_list = []\n",
    "    auth_size_list = []\n",
    "    title_size_list = []\n",
    "    contents_size_list = []\n",
    "    # Acquired target data size\n",
    "    id_data_size = 0\n",
    "    date_data_size = 0\n",
    "    media_data_size = 0\n",
    "    auth_data_size = 0\n",
    "    title_data_size = 0\n",
    "    contents_data_size = 0\n",
    "    # Return dataframe\n",
    "    with open(file, 'r', encoding=DEFAULT_ENCODE, newline='') as f_obj: \n",
    "        csv_reader = csv.reader(f_obj)\n",
    "        header = next(csv_reader)\n",
    "        col_cnt = len(header)\n",
    "        file_name = os.path.basename(f_obj.name)\n",
    "        for row in csv_reader: \n",
    "            row_cnt += 1\n",
    "            i = 0\n",
    "            for col in row:\n",
    "                #print(str(i)+' = '+str(col))\n",
    "                if i == 0: \n",
    "                    id_len_ = len(col)\n",
    "                    id_data_size += id_len_\n",
    "                    id_size_list.append(id_len_)\n",
    "                if i == 1: \n",
    "                    data_len_ = len(col)\n",
    "                    date_data_size += data_len_\n",
    "                    date_size_list.append(data_len_)\n",
    "                if i == 2: \n",
    "                    media_len_ = len(col)\n",
    "                    media_data_size += media_len_\n",
    "                    media_size_list.append(media_len_)\n",
    "                if i == 3: \n",
    "                    auth_len_ = len(col)\n",
    "                    auth_data_size += auth_len_\n",
    "                    auth_size_list.append(auth_len_)                    \n",
    "                if i == 4: \n",
    "                    title_len_ = len(col)\n",
    "                    title_data_size += title_len_\n",
    "                    title_size_list.append(title_len_)\n",
    "                if i == 16:\n",
    "                    contents_len_ = len(col)\n",
    "                    contents_data_size += contents_len_\n",
    "                    contents_size_list.append(contents_len_)\n",
    "                i += 1    \n",
    "            #row_list.append(len(row))\n",
    "            #row_cnt += 1   \n",
    "            \n",
    "        total_data_size = \\\n",
    "            (id_data_size+date_data_size+media_data_size+auth_data_size+title_data_size+contents_data_size) \\\n",
    "            /(1024.0000 * 1024.0000)\n",
    "        \n",
    "        title_data_size_avg = sum(title_size_list)/len(title_size_list)\n",
    "        '''\n",
    "        contents_data_siz_max = max(row_list)\n",
    "        row_min = min(row_list)\n",
    "        '''\n",
    "    # Low data shape\n",
    "    log.info(\"{}: Data shape? Column count={:,.0f}items, Row count={:,.0f}lines\".format(file_name, col_cnt, row_cnt))\n",
    "    # Low data size\n",
    "    log.info(\"{}: Data size? Total={:,.4f}MB=\" \\\n",
    "             .format(file_name, data_size))\n",
    "    # Target data size\n",
    "    log.info(\"{}: Each data size? Total={:,.4f}MB, Target(id={:,.4f}MB, date={:,.4f}MB, media={:,.4f}MB, author={:,.4f}MB, title={:,.4f}MB, contents={:,.4f}MB)\" \\\n",
    "             .format(file_name, \\\n",
    "                     total_data_size, id_data_size/(1024.0000 * 1024.0000), date_data_size/(1024.0000 * 1024.0000), \\\n",
    "                     media_data_size/(1024.0000 * 1024.0000), auth_data_size/(1024.0000 * 1024.0000), \\\n",
    "                     title_data_size/(1024.0000 * 1024.0000), contents_data_size/(1024.0000 * 1024.0000)))\n",
    "    log.info(\"{}: Traget data size? title: (max={:,.4f}MB, min={:,.4f}MB, average={:,.4f}MB, author={:,.4f}MB, title={:,.4f}MB, contents={:,.4f}MB)\" \\\n",
    "             .format(file_name, (np.array(title_size_list)).max(), (np.array(title_size_list)).min()))\n",
    "    log.info(\"{}: Traget data size? contents: (max={:,.0f}MB, min={:,.0f}MB, average={:,.0f}MB, author={:,.4f}MB, title={:,.4f}MB, contents={:,.4f}MB)\" \\\n",
    "             .format(file_name, (np.array(title_size_list)).max(), (np.array(title_size_list)).min()))\n",
    "    \n",
    "csv_file_analysis_support(os.path.join(DEFAULT_PATH, LOW_TRAININGDATA_PATH, DEFAULT_OUTPUT_PATH, LOW_TRAININGDATA4_C19))\n",
    "#    return df, len(df), row_cnt\n",
    "#, row_max, row_min, sum(row_list), len(row_list)\n",
    "\n",
    "'''\n",
    "csv_file_read_support\n",
    "Input: O/S path + file name \n",
    "Output: dataframe['id','date','media','title','contents'], shape, data size, row count, row information (avg, max, min), \n",
    "'''\n",
    "def csv_file_read_support(file):\n",
    "    row_list = []\n",
    "    row_cnt = 0\n",
    "    with open(file, 'r', encoding=DEFAULT_ENCODE, newline='') as f_obj: \n",
    "        # Calculate for each row count, size (avg, max, min)\n",
    "        csv_reader = csv.reader(f_obj)\n",
    "        #row_list = list(csv_reader)\n",
    "        #print(my_data.shape())\n",
    "        # Read header names from the first line of the designated CSV file\n",
    "        header = next(csv_reader)\n",
    "        print(header)\n",
    "        # Just return dataframe by pandas\n",
    "        #df = pd.DataFrame(csv_reader) \n",
    "        for row in csv_reader: \n",
    "\n",
    "            df = pd.DataFrame(row) \n",
    "            if row_cnt > 2:\n",
    "                break\n",
    "            for col in df.iloc[{}]:\n",
    "                print(col)\n",
    "            #row_list.append(len(row))\n",
    "            #row_cnt += 1   \n",
    "        #row_avg = sum(row_list)/len(row_list)\n",
    "        #row_max = max(row_list)\n",
    "        #row_min = min(row_list)\n",
    "#    return df, len(df), row_cnt\n",
    "#, row_max, row_min, sum(row_list), len(row_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps in 2: Model Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1-1: Model Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-12 10:14:40 - thesis_hunyoung_joung - INFO - Low level model data path? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/low_model_data\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Get model data for training\n",
    "Get latest Hangul Wiki data from https://dumps.wikimedia.org/kowiki/latest/ for inital model training\n",
    "'''\n",
    "# Get default low model data using on os path \n",
    "low_model_data_path = os.path.join(DEFAULT_PATH, LOW_TRAININGDATA_PATH, LOW_MODELDATA_PATH).replace(\"\\\\\",\"/\")\n",
    "log.info(\"Low level model data path? {}\".format(low_model_data_path))\n",
    "# Get subdirectory path from default path\n",
    "low_model_data_sub_dir_list = [sub_dir\n",
    "                               for sub_dir in os.listdir(low_model_data_path) \n",
    "                               if os.path.isdir(os.path.join(low_model_data_path, sub_dir))]\n",
    "# Define file count\n",
    "cnt = 0\n",
    "# Open writable text file for the collect the all of wiki files on 'w+' write mode\n",
    "# I/O operation on file use to 'with' function\n",
    "with open(os.path.join(DEFAULT_PATH, LOW_TRAININGDATA_PATH, DEFAULT_OUTPUT_PATH, LOW_MODELDATA), 'w+', encoding=DEFAULT_ENCODE) as output_file: \n",
    "# Aggregate all of the files\n",
    "    for sub_dir in low_model_data_sub_dir_list:\n",
    "        filepath = os.path.join(low_model_data_path, sub_dir).replace(\"\\\\\",\"/\")\n",
    "        file_names = glob(filepath+'\\\\*')\n",
    "        for file_name in file_names:\n",
    "            with open(str(file_name).replace(\"\\\\\",\"/\"), encoding=DEFAULT_ENCODE) as input_file: \n",
    "# Read the data from files and write it in LOW_MODELDATA \n",
    "                #f = input_file.read()\n",
    "                #output_file.write(f)  \n",
    "                for line in input_file:\n",
    "                    output_file.write(line)\n",
    "# Add the line feed\n",
    "            #output_file.write(f) \n",
    "            cnt += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1-2: Model Data Inspection and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-12 10:15:21 - thesis_hunyoung_joung - INFO - Low-level model data file count? 705\n",
      "2020-10-12 10:15:21 - thesis_hunyoung_joung - INFO - Low-level model data size? 707.1919 MB\n",
      "2020-10-12 10:15:21 - thesis_hunyoung_joung - INFO - Low-level model data word count include space? 340,394,582\n",
      "2020-10-12 10:15:21 - thesis_hunyoung_joung - INFO - Low-level model data word count exclude space? 274,202,048\n",
      "2020-10-12 10:15:21 - thesis_hunyoung_joung - INFO - Garbage status? 7\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Exploratory Data Analysis\n",
    "Analyze the model data feature\n",
    "\n",
    "0. Tokenization\n",
    "1. Data size\n",
    "2. Data quantity\n",
    "3. Data words count (include space/exclude space)\n",
    "4. Words frequency visualize\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Define word cloud image name\n",
    "fname = 'model_data_inspection_exploration'\n",
    "# Define word count\n",
    "low_model_data_size = os.path.getsize(os.path.join(DEFAULT_PATH, LOW_TRAININGDATA_PATH, DEFAULT_OUTPUT_PATH, LOW_MODELDATA))/(1024.0 * 1024.0)     \n",
    "# Get words, space length and words volume\n",
    "words, space_len, words_len = file_read_support(os.path.join(DEFAULT_PATH, LOW_TRAININGDATA_PATH, DEFAULT_OUTPUT_PATH, LOW_MODELDATA))\n",
    "# Calculate the collected data size\n",
    "log.info(\"Low-level model data file count? {:,.0f}\".format(cnt))\n",
    "log.info(\"Low-level model data size? {:,.4f}\".format(low_model_data_size)+\" MB\")\n",
    "log.info(\"Low-level model data word count include space? {:,.0f}\".format(words_len+(space_len-1)))\n",
    "log.info(\"Low-level model data word count exclude space? {:,.0f}\".format(words_len))\n",
    "'''\n",
    "w_cloud = WordCloud(width=800, height=600, font_path=DEFAULT_FONT, background_color='white').generate(words)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.axis('off') # Axis remove\n",
    "plt.imshow(w_cloud)\n",
    "plt.savefig(fname, dpi=100)\n",
    "'''\n",
    "# Garbage status\n",
    "log.info(\"Garbage status? {:,.0f}\".format(gc.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncleantext = BeautifulSoup(words, \"lxml\").text\\nw__cloud = WordCloud(width=800, height=600, font_path=DEFAULT_FONT, background_color=\\'white\\').generate(cleantext)\\nplt.figure(figsize=(20,15))\\nplt.axis(\\'off\\') # Axis remove\\nplt.imshow(w__cloud)\\nplt.savefig(fname, dpi=100)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Cleaning & Normalization\n",
    "'''\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Kkma\n",
    "from bs4 import BeautifulSoup\n",
    "'''\n",
    "cleantext = BeautifulSoup(words, \"lxml\").text\n",
    "w__cloud = WordCloud(width=800, height=600, font_path=DEFAULT_FONT, background_color='white').generate(cleantext)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.axis('off') # Axis remove\n",
    "plt.imshow(w__cloud)\n",
    "plt.savefig(fname, dpi=100)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStemming & Lemmatization\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Stemming & Lemmatization\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStopword process\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Stopword process\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRegularization\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Regularization\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nInteger Encoding\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Integer Encoding\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps in 3: Training Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1-1: Training Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-12 10:15:22 - thesis_hunyoung_joung - INFO - Low level trining data path? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data\n",
      "2020-10-12 10:15:22 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/covid19_20191201-20200131-조선.csv\n",
      "2020-10-12 10:15:22 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/covid19_20191201-20200131_20200604-20200630-COVID-19.csv\n",
      "2020-10-12 10:15:22 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/covid19_20191201-20200131_20200604-20200630-COVID.csv\n",
      "2020-10-12 10:15:22 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/covid19_20191201-20200131_20200604-20200630-방송.csv\n",
      "2020-10-12 10:15:22 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/covid19_20191201-20200131_20200604-20200630-우한.csv\n",
      "2020-10-12 10:15:22 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/covid19_20191201-20200131_20200604-20200630-중앙지.csv\n",
      "2020-10-12 10:15:23 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/covid19_20191201-20200603-KBS-MBC-SBS.csv\n",
      "2020-10-12 10:15:23 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/covid19_20191201-20200603-경향-국민-내일-동아-문화.csv\n",
      "2020-10-12 10:15:24 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/covid19_20191201-20200603-디지털타임스-전자신문.csv\n",
      "2020-10-12 10:15:24 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/covid19_20191201-20200603-매일경제-서울경제.csv\n",
      "2020-10-12 10:15:24 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/covid19_20191201-20200603-머니투데이.csv\n",
      "2020-10-12 10:15:25 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/covid19_20191201-20200603-서울-세계-한겨례.csv\n",
      "2020-10-12 10:15:25 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/covid19_20191201-20200603-아시아경제-아주경제.csv\n",
      "2020-10-12 10:15:26 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/covid19_20191201-20200603-조선.csv\n",
      "2020-10-12 10:15:27 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/covid19_20191201-20200603-중앙-한국.csv\n",
      "2020-10-12 10:15:27 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/covid19_20191201-20200603-한국경제-헤럴드경제-파이낸셜뉴스.csv\n",
      "2020-10-12 10:15:28 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/총선_20191201-20200603-KBS-MBC-SBS.csv\n",
      "2020-10-12 10:15:28 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/총선_20191201-20200603-경향-국민-동아-서울-중앙-한겨레.csv\n",
      "2020-10-12 10:15:29 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/총선_20191201-20200603-내일-문화-세계-조선-한국.csv\n",
      "2020-10-12 10:15:30 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/총선_20191201-20200603-디지털타임스-전자신문.csv\n",
      "2020-10-12 10:15:30 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/총선_20191201-20200603-매일경제-머니투데이-서울경제-아시아경제.csv\n",
      "2020-10-12 10:15:30 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/총선_20191201-20200603-아주경제-파이낸셜뉴스-한국경제-헤럴드경제.csv\n",
      "2020-10-12 10:15:31 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/총선_NewsResult_20200604-20200630-21대 선거.csv\n",
      "2020-10-12 10:15:31 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/총선_NewsResult_20200604-20200630-국회의원 선거.csv\n",
      "2020-10-12 10:15:31 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/총선_NewsResult_20200604-20200630-국회의원선거.csv\n",
      "2020-10-12 10:15:31 - thesis_hunyoung_joung - DEBUG - File names? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/총선_NewsResult_20200604-20200630-총선.csv\n",
      "2020-10-12 10:15:40 - thesis_hunyoung_joung - INFO - Garbage status? 0\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Block 2\n",
    "#\n",
    "# Preprocessing 1: file collection\n",
    "# Read all of the article's files that extracted from the 'bigkinds.or.kr '\n",
    "# Add to the defined array COVID-19/Election separately \n",
    "#\n",
    "\n",
    "# Get default low model data using on os path \n",
    "low_training_data_path = os.path.join(DEFAULT_PATH, LOW_TRAININGDATA_PATH).replace(\"\\\\\",\"/\")\n",
    "log.info(\"Low level trining data path? {}\".format(low_training_data_path))\n",
    "# Define array for COVID19\n",
    "covid_list = []\n",
    "# Define array for election\n",
    "election_list = []\n",
    "# File count\n",
    "cnt = 1\n",
    "# Find out all of target files\n",
    "file_names = glob(os.path.join(low_training_data_path, r'*.csv'))\n",
    "# While reading all of the files to detect a file name and to input to each array\n",
    "for file_name in file_names:\n",
    "    log.debug(\"File names? \"+str(file_name).replace(\"\\\\\",\"/\"))\n",
    "    if re.search('covid19', file_name) != None: \n",
    "        covid_list.append(pd.read_csv(file_name, header=None, encoding=DEFAULT_ENCODE))\n",
    "    else:\n",
    "        election_list.append(pd.read_csv(file_name, header=None, encoding=DEFAULT_ENCODE))\n",
    "# Set dataframe to each objective\n",
    "covid_df = pd.concat(covid_list, axis=0, ignore_index=True)\n",
    "election_df = pd.concat(election_list, axis=0, ignore_index=True)\n",
    "# Transfer to CSV within 19 headers\n",
    "'''\n",
    "일자,언론사,기고자,제목,통합 분류1,통합 분류2,통합 분류3,사건/사고 분류1,사건/사고 분류2,사건/사고 분류3,\n",
    "인물,위치,기관,키워드,특성추출,본문,URL,분석제외여부\n",
    "'id','date','media','auth','title','category1','category2','category3','incident1','incident2','incident3',\n",
    "'rel_person','rel_location','rel_org','key_words','feature','contents','url','flag'\n",
    "'''\n",
    "election_df.to_csv(os.path.join(DEFAULT_PATH, LOW_TRAININGDATA_PATH, DEFAULT_OUTPUT_PATH, LOW_TRAININGDATA4_ELECTION), \\\n",
    "                   header=['id','date','media','auth','title','category1','category2','category3','incident1','incident2','incident3',\\\n",
    "                           'rel_person','rel_location','rel_org','key_words','feature','contents','url','flag'], index=False)\n",
    "covid_df.to_csv(os.path.join(DEFAULT_PATH, LOW_TRAININGDATA_PATH, DEFAULT_OUTPUT_PATH, LOW_TRAININGDATA4_C19), \\\n",
    "                header=['id','date','media','auth','title','category1','category2','category3','incident1','incident2','incident3',\\\n",
    "                        'rel_person','rel_location','rel_org','key_words','feature','contents','url','flag'], index=False)\n",
    "# Garbage status\n",
    "log.info(\"Garbage status? {:,.0f}\".format(gc.collect()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1-2: Training Data Inspection and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'date', 'media', 'auth', 'title', 'category1', 'category2', 'category3', 'incident1', 'incident2', 'incident3', 'rel_person', 'rel_location', 'rel_org', 'key_words', 'feature', 'contents', 'url', 'flag']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-379a1b23d3c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mc_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_row_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_row_list\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mcsv_file_read_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEFAULT_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLOW_TRAININGDATA_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEFAULT_OUTPUT_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLOW_TRAININGDATA4_C19\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0me_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_row_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_row_list\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mcsv_file_read_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEFAULT_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLOW_TRAININGDATA_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEFAULT_OUTPUT_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLOW_TRAININGDATA4_ELECTION\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-67335d1a4480>\u001b[0m in \u001b[0;36mcsv_file_read_support\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrow_cnt\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;31m#row_list.append(len(row))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1424\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1426\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2146\u001b[0m         \u001b[1;31m# a list of integers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2147\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2148\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_list_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2150\u001b[0m         \u001b[1;31m# a single integer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2128\u001b[0m         \"\"\"\n\u001b[0;32m   2129\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2131\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2132\u001b[0m             \u001b[1;31m# re-raise with different error message\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[0;32m   3602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3603\u001b[0m         new_data = self._data.take(\n\u001b[1;32m-> 3604\u001b[1;33m             \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3605\u001b[0m         )\n\u001b[0;32m   3606\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   1382\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"int64\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m             \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"int64\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m         )\n\u001b[0;32m   1386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masanyarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \"\"\"\n\u001b[1;32m--> 136\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'dict'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Exploratory Data Analysis\n",
    "Analyze the model data feature\n",
    "\n",
    "0. Tokenization\n",
    "1. Data size\n",
    "2. Data quantity\n",
    "3. Data words count (include space/exclude space)\n",
    "4. Words frequency visualize\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Define word count\n",
    "c_data_size = os.path.getsize(\\\n",
    "            os.path.join(DEFAULT_PATH, LOW_TRAININGDATA_PATH, DEFAULT_OUTPUT_PATH, LOW_TRAININGDATA4_C19))/(1024.0*1024.0)     \n",
    "e_data_size = os.path.getsize(\\\n",
    "            os.path.join(DEFAULT_PATH, LOW_TRAININGDATA_PATH, DEFAULT_OUTPUT_PATH, LOW_TRAININGDATA4_ELECTION))/(1024.0*1024.0)     \n",
    "# Get words, space length and words volume\n",
    "# Return dataframe, data size, row count, row information (avg, max, min)\n",
    "\n",
    "c_df, c_row_count, c_row_list = \\\n",
    "            csv_file_read_support(os.path.join(DEFAULT_PATH, LOW_TRAININGDATA_PATH, DEFAULT_OUTPUT_PATH, LOW_TRAININGDATA4_C19))\n",
    "e_df, e_row_count, e_row_list = \\\n",
    "            csv_file_read_support(os.path.join(DEFAULT_PATH, LOW_TRAININGDATA_PATH, DEFAULT_OUTPUT_PATH, LOW_TRAININGDATA4_ELECTION))\n",
    "print(c_row_count)\n",
    "print(c_row_list)\n",
    "#print(str(max(c_row_list)))\n",
    "#e_df, e_volume, e_row_count, e_row_avg, e_row_max, e_row_min = \\\n",
    "#            csv_file_read_support(os.path.join(DEFAULT_PATH, LOW_TRAININGDATA_PATH, DEFAULT_OUTPUT_PATH, LOW_TRAININGDATA4_ELECTION))\n",
    "#\n",
    "#train_data = pd.read_csv(os.path.join(DEFAULT_PATH, LOW_TRAININGDATA_PATH, DEFAULT_OUTPUT_PATH, LOW_TRAININGDATA4_C19), header=0, delimiter=\"\\t\", quoting=3)\n",
    "'''\n",
    "train_length = train_data[3].apply(len)\n",
    "train_length.head()\n",
    "train_fname = 'training_data_inspection_exploration'\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.hist(train_length, bins=200, alpha=0.5, color='r', label='word')\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.title('Log=Histogram of length of review')\n",
    "plt.xlabel('Length of review')\n",
    "plt.ylabel('Number of review')\n",
    "#plt.imshow(w_cloud)\n",
    "plt.savefig(train_fname, dpi=100)\n",
    "\n",
    "# File volume   \n",
    "log.info(\"Training data size? COVID-19={:,.4f}MB, 21th General Election={:,.4f}MB\".format(c_data_size, e_data_size))\n",
    "# File count        \n",
    "log.info(\"Each Training data count? COVID-19={:,.0f}ea, 21th General Election={:,.0f}ea\".format(len(covid_list), len(election_list)))\n",
    "# Files row count\n",
    "log.info(\"Each Training data row count? COVID-19={:,.0f}row, 21th General Election={:,.0f}row\".format(len(covid_df), len(election_df)))\n",
    "# Words count   \n",
    "log.info(\"Training data word count include space? COVID-19={:,.0f}, 21th General Election={:,.0f}\".format(c_words_len+(e_space_len-1), e_words_len+(e_space_len-1)))\n",
    "log.info(\"Training data word count exclude space? COVID-19={:,.0f}, 21th General Election={:,.0f}\".format(c_words_len, e_words_len))\n",
    "\n",
    "\n",
    "#for i, j in election_df.iterrows():\n",
    "#    print(i, j) \n",
    "\n",
    "# Define word cloud image name\n",
    "fname = 'model_data_inspection_exploration'\n",
    "# Define word count\n",
    "low_model_data_size = os.path.getsize(os.path.join(low_model_data_path, LOW_MODELDATA))/(1024.0 * 1024.0)     \n",
    "# Get words, space length and words volume\n",
    "words, space_len, words_len = file_read_support(os.path.join(low_model_data_path, LOW_MODELDATA))\n",
    "# Calculate the collected data size\n",
    "log.info(\"Low-level model data file count? {:,.2f}\".format(cnt))\n",
    "log.info(\"Low-level model data size? {:,.2f}\".format(low_model_data_size)+\" MB\")\n",
    "log.info(\"Low-level model data word count include space? {:,.2f}\".format(words_len+(space_len-1)))\n",
    "log.info(\"Low-level model data word count exclude space? {:,.2f}\".format(words_len))\n",
    "\n",
    "w_cloud = WordCloud(width=800, height=600, font_path=DEFAULT_FONT, background_color='white').generate(words)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.axis('off') # Axis remove\n",
    "plt.imshow(w_cloud)\n",
    "plt.savefig(fname, dpi=100)\n",
    "\n",
    "# Garbage status\n",
    "log.info(\"Garbage status? {:,.2f}\".format(gc.collect()))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Cleaning & Normalization\n",
    "'''\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Kkma\n",
    "from bs4 import BeautifulSoup\n",
    "'''\n",
    "cleantext = BeautifulSoup(words, \"lxml\").text\n",
    "w__cloud = WordCloud(width=800, height=600, font_path=DEFAULT_FONT, background_color='white').generate(cleantext)\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.axis('off') # Axis remove\n",
    "plt.imshow(w__cloud)\n",
    "plt.savefig(fname, dpi=100)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Block 3\n",
    "#\n",
    "# Preprocessing 2: Tokenization\n",
    "# 모델데이터가 작은 관계로 미리 학습된 모델데이터를 획득. \n",
    "# https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-14%EC%9D%BC%EC%B0%A8-word2vec-%EC%8B%A4%EC%8A%B5-a4e7767a1e66\n",
    "# 위키데이터를 획득후, xml형식을 택스트로 변환, 훈련모델데이터를 구축하였다.->검증->시험\n",
    "# Training data load\n",
    "#\n",
    "import csv\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "\n",
    "training_data_set = []\n",
    "test_data_set = []\n",
    "tr = list(csv.reader(row_date['contents'].head(), delimiter=','))\n",
    "print(len(tr)) # 리뷰 개수 출력\n",
    "\n",
    "for item in tr:\n",
    "   training_data_set.append(Kkma.morphs(str(item)))\n",
    "    \n",
    "# Define test and training input concatenation\n",
    "#est_data_set = pd.concat([row_date['contents'],row_date['title']],axis=0)\n",
    "\n",
    "# Vectorize complete data\n",
    "#vectorizer = CountVectorizer()\n",
    "#vectorizer.fit(test_data_set)\n",
    "\n",
    "# Print all unique tokens used in overall dataset\n",
    "#nique_tokens = vectorizer.get_feature_names()\n",
    "#rint(unique_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training_data_set model\n",
    "# Vector=100 demention, window=+- 1, frequency =5, CPU = quard , tranning count=80, Skip Gram\n",
    "model = Word2Vec(training_data_set, size=10, alpha=0.025, window=10, min_count=50, sg=0, workers=4, iter=80)\n",
    "s1 = '국정운영'\n",
    "#s2 = ''\n",
    "# Save model\n",
    "model.save(\"word2vec.model\")\n",
    "# test similarity vector (top 100)\n",
    "print(\"Test similarity top 100? \", model.most_similar(s1, topn=100))\n",
    "wvs = model.wv\n",
    "# Vocabularies\n",
    "vocabs = wvs.vocab.keys()\n",
    "wv_list = [wvs[v] for v in vocabs]\n",
    "print(\"Vocabularies? \", vocabs)\n",
    "#print(\"Test similarity? \", wvs.similarity(w1=(s1), w2=(s2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show similarity graph \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define graph function\n",
    "def plot_array(vocabs, x, y):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(x, y)\n",
    "    for i, v in enumerate(vocabs):\n",
    "        plt.annotate(v, xy=(x[i], y[i]))\n",
    "        \n",
    "pca = PCA(n_components=10)\n",
    "xys = pca.fit_transform(wv_list)\n",
    "x = xys[:,0]\n",
    "y = xys[:,1]\n",
    "        \n",
    "plot_array(vocabs, x, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "model.train(unique_tokens, total_examples=1, epochs=1)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "xys = pca.fit_transform( [wvs[v] for v in model.wv.vocab.keys()])\n",
    "x = xys[:,0]\n",
    "y = xys[:,1]\n",
    "        \n",
    "plot_array(vocabs, x, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training input and output\n",
    "X=vectorizer.transform(training_data['input'])\n",
    "y=training_data['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "clf_svc = SVC()\n",
    "\n",
    "scoring = ['accuracy','f1']\n",
    "scores = cross_validate(clf_svc, X, y, scoring=scoring,cv=10, return_train_score=False)\n",
    "\n",
    "print(\"mean accuracy : {}\".format(scores['test_accuracy'].mean()))\n",
    "print(\"mean f1 score : {}\".format(scores['test_f1'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
