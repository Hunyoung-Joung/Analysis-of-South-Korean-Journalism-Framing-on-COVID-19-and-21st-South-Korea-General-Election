{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 프로젝트는 포탈사이트를 통하여, 각 미디어의 기사를 추출하고, 각 기사들의 유사도를 분석하는 프로젝트이다.\n",
    "초기의 모델데이터가 존재하지 않는 관계로 기사내용을 Tokenizing을 통해 추출하고, 이를 Model data로 구축하였으며,\n",
    "Test dataset을 훈련시킨후, 이에 대한 유사도의 Accuracy를 높이려고 하였다.\n",
    "또한, Model의 크기가, Test dataset의 크기보다 커서, 향상된 Accuracy를 크게 기대하지는 않았다.\n",
    "\n",
    "이를 위하여 포탈사이트인 다음의 뉴스 Page 들을 (https://media.daum.net/) Crawling하여, 각 미디어사 및 주제별 등으로 Low data를 취득하였다. \n",
    "단계는 다음과 같습니다.\n",
    "\n",
    "1. Web Crawling\n",
    " Java를 이용하여, 아래와 같이 기사들을 취합하였다.\n",
    " https://github.com/Hunyoung-Joung/MorphologicalAnalysisWithRNN\n",
    " \n",
    " 1.1. Source of Crawling\n",
    "   포탈 사이트인 다음의 뉴스기사들의 Seed URI는 아래 *2.1. 와 같으며, 깊이검색은 5단계까지, 넓이검색은 하지 않는다. \n",
    "   또한, 아래의 *2.2. 에 해당하는 페이지들은 제외하여, Text data들만 취합하였다.\n",
    "\n",
    "    1.1.1. Seed URI\n",
    "    ①\thttps://media.daum.net/foreign/\n",
    "    ②\thttps://media.daum.net/economic/\n",
    "    ③\thttps://media.daum.net/society/\n",
    "    ④\thttps://media.daum.net/politics/\n",
    "    ⑤\thttps://media.daum.net/culture/\n",
    "    ⑥\thttps://media.daum.net/digital/\n",
    "\n",
    "    1.1.2. 아래와 같은 형식의 페이지는 제외 하였다.\n",
    "    ①\tStyle sheet page\n",
    "    ①\tScript page\n",
    "    ②\tZipped page\n",
    "    ③\tVideo page\n",
    "    ④\tAudio page \n",
    "    \n",
    " 1.2. Low data취득\n",
    "  1.2.1. Data format\n",
    "   Low data: {uri}\\t||{category}\\t||{publisher}\\t||{subject}\\t||{content}\n",
    "   crawl_result.tsv\n",
    "   \n",
    "2. Data model구축\n",
    " Jupyter에서 Python을 이용하여 초기 Model을 구축하였고, 최소 1번 Keyword를 훈련 하였다. \n",
    " 또한, log는 최소한으로 표시한다.\n",
    " \n",
    " 2.1. Library 취득\n",
    "  System library를 통하여, jpype를 비롯한 각각의 필요한 Library들을 취득하였다.    \n",
    "  취득한 Library를 import하고, 필요한 함수들을 정의하였다.\n",
    "  \n",
    "  \n",
    "  \n",
    " 2.2. Low data 취득\n",
    "  Pandas를 이용하여, \"./crawl_result.tsv\"를 취득하여, Low dataset을 작성하였고, 각각의 데이터에 Head명을 부여하였다.\n",
    "  \n",
    " 2.3. Tokenizing & Training dataset & Test dataset 작성\n",
    "  konlpy를 이용하여, 각각의 Data에서 Token을 추출하고,\n",
    "  Low dataset의 \"contents\"를 Training dataset으로 작성하였고,\n",
    "  Low dataset의 \"title\"+\"contents\"를 Test dataset으로 작성하였다.\n",
    "  한편, Low data취득시에 Separator로 사용한 \"||\"를 제거하였다.\n",
    " \n",
    " 2.4. Model 작성\n",
    "  Tranning dataset을 Word2Vector를 사용하여 아래와 같이 작성하였다.\n",
    "  Vector크기=100차원\n",
    "  Window=좌우 5 Token까지\n",
    "  frequency=5회반복\n",
    "  CPU=4개 사용  \n",
    "  Training count=80회반복\n",
    "  초기 Model이 Test dataset보다 커서, Common Bag Of Words CBOW Model대신, Skip Gram을 사용하였다.\n",
    " \n",
    " 2.5. 훈련전 검증 및 시각화\n",
    "  Model dataset을 작성하였고, \n",
    "  초기Model과 훈련후의 Test dataset의 Accuracy 를 비교하기 위해, \n",
    "  이에 대한 Sample similarity을 출력해보았으며, 시각화하였다.\n",
    " \n",
    " 2.7. Training\n",
    "  Model dataset에 Vector화된 Test dataset를 추가하였다.\n",
    "  \n",
    " 2.8. 훈련후 검증 및 시각화\n",
    "   - SVM\n",
    "   \n",
    "3. Conclusion    \n",
    " Model 학습 후 유효성 검사 세트의 정확성\n",
    "   - SVM\n",
    "   \n",
    "첫번째 고민, 기계가 인간의 의도를 이해하여야 프레임을 추출 할 수 있는 것인가?\n",
    "그렇다면, 이해해야할 기사의 쟁점을 이해해야 하는 가, 아니면 \n",
    "\n",
    "인간: 내용의 이해 -> 중심주장을 추출 -> 의도의 파악\n",
    "기계: 문장의 중심을 추출 -> 구조의 파악\n",
    "\n",
    "두번째 고민, 각 프레임 분석과 기계분석을 대입가능한지 여부 판독.\n",
    "bert에 대한 고민..\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "https://github.com/Hunyoung-Joung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps in 1: Initiative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anaconda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1.1)\n",
      "Requirement already satisfied: gensim in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.19.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.9.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied: boto3 in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.10.19)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (1.13.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.14.0,>=1.13.19->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.14.0,>=1.13.19->boto3->smart-open>=1.8.1->gensim) (0.14)\n",
      "Requirement already satisfied: anaconda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1.1)\n",
      "Requirement already satisfied: BeautifulSoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from BeautifulSoup4) (1.8)\n",
      "Requirement already satisfied: anaconda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1.1)\n",
      "Requirement already satisfied: konlpy in c:\\programdata\\anaconda3\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: JPype1>=0.5.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in c:\\programdata\\anaconda3\\lib\\site-packages (from JPype1>=0.5.7->konlpy) (3.7.4.2)\n",
      "Requirement already satisfied: anaconda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.19.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
      "Requirement already satisfied: anaconda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.19.1)\n",
      "Requirement already satisfied: anaconda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
      "Requirement already satisfied: anaconda in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (0.21.2)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.19.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (0.13.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-01 02:54:14 - thesis_hunyoung_joung - INFO - BEGIN ################################################################################\n",
      "2020-10-01 02:54:14 - thesis_hunyoung_joung - INFO - Default path? C:\\Users\\root\\Documents\\0. The Cyber University of Korea\\Graduated School\\Semester 3\\Thesis\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Get libraries and install\n",
    "\n",
    "Define libraries and make alias\n",
    "Define global variable as like final or static\n",
    "'''\n",
    "import sys\n",
    "!{sys.executable} -m pip install anaconda gensim\n",
    "!{sys.executable} -m pip install anaconda BeautifulSoup4\n",
    "!{sys.executable} -m pip install anaconda konlpy\n",
    "!{sys.executable} -m pip install anaconda pandas\n",
    "!{sys.executable} -m pip install anaconda numpy\n",
    "!{sys.executable} -m pip install anaconda pandas\n",
    "!{sys.executable} -m pip install anaconda scikit-learn\n",
    "\n",
    "import os \n",
    "import gc\n",
    "import jpype\n",
    "import konlpy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import pandas\n",
    "import gensim\n",
    "import logging\n",
    "import warnings\n",
    "import codecs\n",
    "import re\n",
    "import glob\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "global DEFAULT_PATH           # Default file path\n",
    "global LOW_MODELDATA_PATH     # Low level model data path\n",
    "global LOW_TRAININGDATA_PATH  # Low level training data path\n",
    "global LOW_MODELDATA          # Low level training data file\n",
    "global default_encoding       # For byte\n",
    "global hangul_encoding        # For file name and contents\n",
    "global jap_encoding           # For OS path\n",
    "global covid_df               # COVID19 dataframe\n",
    "global election_df            # Election dataframe\n",
    "global LINE_FEED              # Line feed\n",
    "\n",
    "# Default paths for Japanese Windows version\n",
    "DEFAULT_PATH = r'C:\\Users\\root\\Documents\\0. The Cyber University of Korea\\Graduated School\\Semester 3\\Thesis'\n",
    "LOW_TRAININGDATA_PATH = 'data'\n",
    "LOW_MODELDATA_PATH = 'low_model_data'\n",
    "LOW_MODELDATA = 'model_data.txt'\n",
    "LINE_FEED = '\\n'\n",
    "\n",
    "# Default encoding\n",
    "DEFAULT_ENCODE = 'UTF-8' \n",
    "# Encode for Hangul 'cp949'\n",
    "hangul_encoding = 'cp949'\n",
    "# Encode for Japnese 'cp932'\n",
    "jap_encoding = 'cp932'\n",
    "\n",
    "# Set log\n",
    "log = logging.getLogger(\"thesis_hunyoung_joung\")\n",
    "log.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s',\"%Y-%m-%d %H:%M:%S\")\n",
    "log.propagate = True\n",
    "# Output to console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(formatter)\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "# Output to file\n",
    "file_handler = logging.FileHandler(filename=\"thesis_hunyoung_joung_run.log\")\n",
    "file_handler.setFormatter(formatter)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "# Record logs to file and show to console\n",
    "log.addHandler(console_handler)\n",
    "log.addHandler(file_handler)\n",
    "# 80 length\n",
    "log.info(\"BEGIN ################################################################################\")\n",
    "log.info(\"Default path? {}\".format(DEFAULT_PATH))\n",
    "\n",
    "#class FileReadSupport:\n",
    "def file_read_support(file):\n",
    "    with open(file, 'r', encoding=DEFAULT_ENCODE) as f: \n",
    "        data = f.read(256)\n",
    "        words = data.split()\n",
    "        words_len = 0\n",
    "        for i in words:\n",
    "            words_len += len(i)\n",
    "    return words, len(words), words_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps in 2: Model Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1-1: Model Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-01 02:54:14 - thesis_hunyoung_joung - INFO - Low level model data path? C:/Users/root/Documents/0. The Cyber University of Korea/Graduated School/Semester 3/Thesis/data/low_model_data\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Get model data for training\n",
    "Get latest Hangul Wiki data from https://dumps.wikimedia.org/kowiki/latest/ for inital model training\n",
    "'''\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "# Get default low model data using on os path \n",
    "low_model_data_path = os.path.join(DEFAULT_PATH, LOW_TRAININGDATA_PATH, LOW_MODELDATA_PATH).replace(\"\\\\\",\"/\")\n",
    "log.info(\"Low level model data path? {}\".format(low_model_data_path))\n",
    "# Get subdirectory path from default path\n",
    "low_model_data_sub_dir_list = [sub_dir\n",
    "                               for sub_dir in os.listdir(low_model_data_path) \n",
    "                               if os.path.isdir(os.path.join(low_model_data_path, sub_dir))]\n",
    "# Define file count\n",
    "cnt = 0\n",
    "# Open writable text file for the collect the all of wiki files on 'w+' write mode\n",
    "# I/O operation on file use to 'with' function\n",
    "with open(os.path.join(low_model_data_path, LOW_MODELDATA), 'w+', encoding=DEFAULT_ENCODE) as output_file: \n",
    "# Aggregate all of the files\n",
    "    for sub_dir in low_model_data_sub_dir_list:\n",
    "        filepath = os.path.join(low_model_data_path, sub_dir).replace(\"\\\\\",\"/\")\n",
    "        file_names = glob(filepath+'\\\\*')\n",
    "        for file_name in file_names:\n",
    "            with open(str(file_name).replace(\"\\\\\",\"/\"), encoding=DEFAULT_ENCODE) as input_file: \n",
    "# Read the data from files and write it in LOW_MODELDATA \n",
    "                f = input_file.read(256)\n",
    "                output_file.write(f)  \n",
    "# Add the line feed\n",
    "            output_file.write(f) \n",
    "            cnt += 1\n",
    "            input_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1-2: Model Data Inspection and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-17374ac69ae3>, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-17374ac69ae3>\"\u001b[1;36m, line \u001b[1;32m32\u001b[0m\n\u001b[1;33m    log.info(\"Low-level model data word count exclude space? {}\".format(words_len))\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Exploratory Data Analysis\n",
    "Analyze the model data feature\n",
    "\n",
    "1. Data size\n",
    "2. Data quantity\n",
    "3. Data words count (include space/exclude space)\n",
    "\n",
    "3. 각 리뷰의 문자 길이 분포\n",
    "\n",
    "4. 많이 사용되는 단어\n",
    "5. 긍정/부정 데이터 분포\n",
    "\n",
    "6. 각 리뷰의 단어 개수 분포\n",
    "\n",
    "7. 특수문자 및 대분자, 소문자 비율\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define word count\n",
    "#word_count = 0\n",
    "low_model_data_size = os.path.getsize(os.path.join(low_model_data_path, LOW_MODELDATA))/(1024.0 * 1024.0 * 1024.0)     \n",
    "\n",
    "data, space_len, words_len = file_read_support(os.path.join(low_model_data_path, LOW_MODELDATA))\n",
    "# Calculate the collected data size\n",
    "log.info(\"Low-level model data file count? {}\".format(cnt))\n",
    "log.info(\"Low-level model data size? {}\".format(low_model_data_size)+\" GB\")\n",
    "log.info(\"Low-level model data word count include space? {}\".format(words_len+(space_len-1)))\n",
    "log.info(\"Low-level model data word count exclude space? {}\".format(words_len))\n",
    "'''\n",
    "with open(os.path.join(low_model_data_path, LOW_MODELDATA), 'r', encoding=DEFAULT_ENCODE) as f: \n",
    "    data = f.read()\n",
    "    words = data.split()\n",
    "    log.info(\"Low level model data word count? {}\".format(len(words)))\n",
    "    f.close()\n",
    "'''\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "#model_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3: Model Data Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Block 2\n",
    "#\n",
    "# Preprocessing 1: file collection\n",
    "# Read all of the article's files that extracted from the 'bigkinds.or.kr '\n",
    "# Add to the defined array COVID-19/Election separately \n",
    "#\n",
    "import pandas as pd\n",
    "import re\n",
    "from glob import glob\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "# Define array for COVID19\n",
    "covid_list = []\n",
    "# Define array for election\n",
    "election_list = []\n",
    "# File count\n",
    "cnt = 1\n",
    "# Find out all of target files\n",
    "file_names = glob(r'*.csv')\n",
    "# While reading all of the files to detect a file name and to input to each array\n",
    "for file_name in file_names:\n",
    "    # TODO\n",
    "    log.debug(\"File names? \"+str(file_name).replace(\"\\\\\",\"/\"))\n",
    "    if re.search('covid19', file_name) != None: \n",
    "        covid_list.append(pd.read_csv(file_name, header=None, index_col=0, encoding=default_encoding))\n",
    "    else:\n",
    "        election_list.append(pd.read_csv(file_name, header=None, index_col=0, encoding=default_encoding))\n",
    "# Confirm file count        \n",
    "log.info(\"File count of relevant with COVID-19? \"+str(len(covid_list)))\n",
    "log.info(\"File count of relevant with election? \"+str(len(election_list)))\n",
    "# Set dataframe to each objective\n",
    "covid_df = pd.concat(covid_list, axis=0, ignore_index=True)\n",
    "election_df = pd.concat(election_list, axis=0, ignore_index=True)\n",
    "# Items, the total count is 19 EA\n",
    "\"\"\"\n",
    "일자,언론사,기고자,제목,통합 분류1,통합 분류2,통합 분류3,\n",
    "사건/사고 분류1,사건/사고 분류2,사건/사고 분류3,인물,위치,기관,키워드,특성추출,본문,URL,분석제외여부\n",
    "\n",
    "Datafrmae structure ['id','date','media','auth','title','category1','category2','category3',\n",
    "'incident1','incident2','incident3','rel_person','rel_location','rel_org','key_words','feature','contents','url','flag']\n",
    "\"\"\"\n",
    "covid_df = pd.concat(covid_list, axis=0, ignore_index=True)\n",
    "election_df = pd.concat(election_list, axis=0, ignore_index=True)\n",
    "# Check each files row count\n",
    "log.info(\"Row count of relevant with COVID-19? \"+str(len(covid_df)))\n",
    "log.info(\"Row count of relevant with election? \"+str(len(election_df))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Block 3\n",
    "#\n",
    "# Preprocessing 2: Tokenization\n",
    "# 모델데이터가 작은 관계로 미리 학습된 모델데이터를 획득. \n",
    "# https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-14%EC%9D%BC%EC%B0%A8-word2vec-%EC%8B%A4%EC%8A%B5-a4e7767a1e66\n",
    "# 위키데이터를 획득후, xml형식을 택스트로 변환, 훈련모델데이터를 구축하였다.->검증->시험\n",
    "# Training data load\n",
    "#\n",
    "import csv\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "\n",
    "training_data_set = []\n",
    "test_data_set = []\n",
    "tr = list(csv.reader(row_date['contents'].head(), delimiter=','))\n",
    "print(len(tr)) # 리뷰 개수 출력\n",
    "\n",
    "for item in tr:\n",
    "   training_data_set.append(Kkma.morphs(str(item)))\n",
    "    \n",
    "# Define test and training input concatenation\n",
    "#est_data_set = pd.concat([row_date['contents'],row_date['title']],axis=0)\n",
    "\n",
    "# Vectorize complete data\n",
    "#ectorizer = CountVectorizer()\n",
    "#ectorizer.fit(test_data_set)\n",
    "\n",
    "# Print all unique tokens used in overall dataset\n",
    "#nique_tokens = vectorizer.get_feature_names()\n",
    "#rint(unique_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training_data_set model\n",
    "# Vector=100 demention, window=+- 1, frequency =5, CPU = quard , tranning count=80, Skip Gram\n",
    "model = Word2Vec(training_data_set, size=10, alpha=0.025, window=10, min_count=50, sg=0, workers=4, iter=80)\n",
    "s1 = '국정운영'\n",
    "#s2 = ''\n",
    "# Save model\n",
    "model.save(\"word2vec.model\")\n",
    "# test similarity vector (top 100)\n",
    "print(\"Test similarity top 100? \", model.most_similar(s1, topn=100))\n",
    "wvs = model.wv\n",
    "# Vocabularies\n",
    "vocabs = wvs.vocab.keys()\n",
    "wv_list = [wvs[v] for v in vocabs]\n",
    "print(\"Vocabularies? \", vocabs)\n",
    "#print(\"Test similarity? \", wvs.similarity(w1=(s1), w2=(s2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show similarity graph \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define graph function\n",
    "def plot_array(vocabs, x, y):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(x, y)\n",
    "    for i, v in enumerate(vocabs):\n",
    "        plt.annotate(v, xy=(x[i], y[i]))\n",
    "        \n",
    "pca = PCA(n_components=10)\n",
    "xys = pca.fit_transform(wv_list)\n",
    "x = xys[:,0]\n",
    "y = xys[:,1]\n",
    "        \n",
    "plot_array(vocabs, x, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "model.train(unique_tokens, total_examples=1, epochs=1)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "xys = pca.fit_transform( [wvs[v] for v in model.wv.vocab.keys()])\n",
    "x = xys[:,0]\n",
    "y = xys[:,1]\n",
    "        \n",
    "plot_array(vocabs, x, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training input and output\n",
    "X=vectorizer.transform(training_data['input'])\n",
    "y=training_data['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "clf_svc = SVC()\n",
    "\n",
    "scoring = ['accuracy','f1']\n",
    "scores = cross_validate(clf_svc, X, y, scoring=scoring,cv=10, return_train_score=False)\n",
    "\n",
    "print(\"mean accuracy : {}\".format(scores['test_accuracy'].mean()))\n",
    "print(\"mean f1 score : {}\".format(scores['test_f1'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
